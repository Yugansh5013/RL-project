{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the important packeges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import os\n",
    "import logging\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_buffer=100_000\n",
    "batch_size=64\n",
    "gamma=0.99  #discount factor\n",
    "alpha=0.001 #learning rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Details\n",
    "\n",
    "## Action Space\n",
    "| Action | Description | Unit |\n",
    "|--------|-------------|------|\n",
    "| 0 | Apply -1 torque to the actuated joint | torque (N m) |\n",
    "| 1 | Apply 0 torque to the actuated joint | torque (N m) |\n",
    "| 2 | Apply 1 torque to the actuated joint | torque (N m) |\n",
    "\n",
    "## Observation Space\n",
    "| Index | Observation | Min | Max |\n",
    "|-------|-------------|-----|-----|\n",
    "| 0 | Cosine of theta1 | -1 | 1 |\n",
    "| 1 | Sine of theta1 | -1 | 1 |\n",
    "| 2 | Cosine of theta2 | -1 | 1 |\n",
    "| 3 | Sine of theta2 | -1 | 1 |\n",
    "| 4 | Angular velocity of theta1 | ~ -12.567 (-4 * pi) | ~ 12.567 (4 * pi) |\n",
    "| 5 | Angular velocity of theta2 | ~ -28.274 (-9 * pi) | ~ 28.274 (9 * pi) |\n",
    "\n",
    "### Angle Definitions\n",
    "- **theta1**: Angle of the first joint (0° = pointing directly downwards)\n",
    "- **theta2**: Angle relative to the first link (0° = same angle between links)\n",
    "\n",
    "Angular velocities are bounded at ±4π and ±9π rad/s for theta1 and theta2 respectively.\n",
    "A state of [1, 0, 1, 0, ..., ...] indicates both links pointing downwards.\n",
    "\n",
    "## Rewards\n",
    "- Goal: Reach target height in minimum steps\n",
    "- Regular step: -1 reward\n",
    "- Goal achieved: 0 reward and termination\n",
    "- Reward threshold: -100\n",
    "\n",
    "## Starting State\n",
    "All parameters (theta1, theta2, and angular velocities) are initialized uniformly between -0.1 and 0.1, resulting in both links pointing downwards with slight randomness.\n",
    "\n",
    "## Episode Termination Conditions\n",
    "1. **Success**: Free end reaches target height\n",
    "   - Condition: -cos(theta1) - cos(theta2 + theta1) > 1.0\n",
    "2. **Truncation**: Episode length exceeds 500 steps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yugan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:416: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#this is to render the initial position of the environment\n",
    "env.reset()\n",
    "state=env.reset()\n",
    "image=env.render()\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yugan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\yugan\\OneDrive\\Documents\\python\\RL project\\video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "#this is to render a video of the environment\n",
    "env = RecordVideo(env, video_folder='./video', episode_trigger=lambda x: x % 1 == 0)\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # Take a random action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Ensure rendering happens\n",
    "    env.render()\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9982241   0.05957125  0.9993851   0.03506178 -0.21309187  0.4617512 ]\n",
      "-1.0\n",
      "False\n",
      "False\n",
      "{}\n",
      "(array([ 0.99679434,  0.0800065 ,  0.9998612 , -0.01666118,  0.01431314,\n",
      "        0.04290292], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "state=env.reset()\n",
    "action=2\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "print(observation)\n",
    "print(reward)\n",
    "print(terminated)\n",
    "print(truncated)\n",
    "print(info)\n",
    "print(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initlizing the q and target q network\n",
    "the structure of both the networks will be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnetwork=Sequential([\n",
    "    Input(shape=(6,)), #input layer consist of 6 observations\n",
    "    Dense(128, activation=\"relu\"), #hidden layer 1\n",
    "    Dense(128, activation=\"relu\"), #hidden layer 2\n",
    "    Dense(3, activation=\"linear\") # 3 is the number of actions\n",
    "])\n",
    "\n",
    "target_qnetwork=Sequential([\n",
    "    Input(shape=(6,)),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(3, activation=\"linear\")\n",
    "])\n",
    "\n",
    "# Initlizing the optimizer\n",
    "optimizer=Adam(learning_rate=alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a named tuple called experienced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience=namedtuple(\"experience\",[\"state\",\"action\",\"reward\",\"next_state\",\"done\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the loss function\n",
    "def compute_loss(experiences,gamma,qnetwork,target_qnetwork):\n",
    "    states,actions,rewards,next_states,done_vals=experiences\n",
    "    max_qsa=tf.reduce_max(target_qnetwork(next_states),axis=1)\n",
    "    y_targets=rewards+gamma*max_qsa*(1-done_vals)\n",
    "    q_values=qnetwork(states)\n",
    "    actions=tf.cast(actions,tf.int32)\n",
    "    q_values=tf.gather_nd(q_values,tf.stack([tf.range(batch_size),actions],axis=1))\n",
    "    loss = tf.reduce_mean(tf.square(y_targets - q_values))\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the function to actually train the model\n",
    "\n",
    "TAU=0.001 #soft update parameter\n",
    "\n",
    "@tf.function\n",
    "def train_step(experiences,gamma):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss=compute_loss(experiences,gamma,qnetwork,target_qnetwork)\n",
    "    model_gradients=tape.gradient(loss,qnetwork.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(model_gradients,qnetwork.trainable_variables))\n",
    "    #updating the target q network\n",
    "    #target_qnetwork.set_weights(qnetwork.get_weights()) -- this is not optimal, this is also called hard update\n",
    "    #instead we will soft update the target q network\n",
    "    target_qnetwork.set_weights(\n",
    "        tf.keras.utils.serialize_keras_object(qnetwork.get_weights())\n",
    "    )\n",
    "    for target_weights, q_net_weights in zip(target_qnetwork.trainable_variables, qnetwork.trainable_variables):\n",
    "        target_weights.assign(TAU * q_net_weights + (1.0 - TAU) * target_weights)  #this means that the target network will be updated with a small step towards the q network\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the e-greedy policy\n",
    "epsilon=1.0\n",
    "E_MIN=0.01\n",
    "E_DECAY=0.995\n",
    "\n",
    "#function to choose the action\n",
    "def get_action(qvalues,epsilon=0):\n",
    "    if np.random.rand()<epsilon:\n",
    "        action=np.random.choice(np.arange(3))\n",
    "    else:\n",
    "        action=np.argmax(qvalues)\n",
    "    return action\n",
    "def get_new_eps(epsilon):\n",
    "    return max(E_MIN, E_DECAY*epsilon)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils for the coustom training loops\n",
    "def check_update_conditions(t, num_steps_upd, memory):\n",
    "    if (t + 1) % num_steps_upd == 0 and len(memory) > batch_size:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=batch_size)\n",
    "    states = tf.convert_to_tensor(np.array([e.state[0] for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(np.array([e.action for e in experiences if e is not None],dtype=np.int32), dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(np.array([e.next_state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    done_vals = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.float32),dtype=tf.float32)\n",
    "    return (states, actions, rewards, next_states, done_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Deep Q-Learning Algorithm with Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yugan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\yugan\\OneDrive\\Documents\\python\\RL project\\acrobat-agent folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\yugan\\AppData\\Local\\Temp\\ipykernel_24904\\3256661627.py\", line 14, in train_step  *\n        target_qnetwork.set_weights(\n    File \"c:\\Users\\yugan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 693, in get_weights  **\n        return [v.numpy() for v in self.weights]\n    File \"c:\\Users\\yugan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 693, in <listcomp>\n        return [v.numpy() for v in self.weights]\n    File \"c:\\Users\\yugan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py\", line 71, in numpy\n        return self.value.numpy()\n\n    NotImplementedError: numpy() is only available when eager execution is enabled.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_update_conditions(t,\u001b[38;5;241m5\u001b[39m, memory):\n\u001b[0;32m     33\u001b[0m     experiences\u001b[38;5;241m=\u001b[39mget_experiences(memory)\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m state\u001b[38;5;241m=\u001b[39mnext_state\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32mc:\\Users\\yugan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filep3o3s45x.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(experiences, gamma)\u001b[0m\n\u001b[0;32m     10\u001b[0m model_gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(qnetwork)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(model_gradients), ag__\u001b[38;5;241m.\u001b[39mld(qnetwork)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 12\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(target_qnetwork)\u001b[38;5;241m.\u001b[39mset_weights, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mserialize_keras_object, (\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqnetwork\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_state\u001b[39m():\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n",
      "File \u001b[1;32mc:\\Users\\yugan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:693\u001b[0m, in \u001b[0;36mLayer.get_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    692\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the values of `layer.weights` as a list of NumPy arrays.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [v\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights]\n",
      "File \u001b[1;32mc:\\Users\\yugan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:693\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    692\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the values of `layer.weights` as a list of NumPy arrays.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights]\n",
      "File \u001b[1;32mc:\\Users\\yugan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:71\u001b[0m, in \u001b[0;36mVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"C:\\Users\\yugan\\AppData\\Local\\Temp\\ipykernel_24904\\3256661627.py\", line 14, in train_step  *\n        target_qnetwork.set_weights(\n    File \"c:\\Users\\yugan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 693, in get_weights  **\n        return [v.numpy() for v in self.weights]\n    File \"c:\\Users\\yugan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 693, in <listcomp>\n        return [v.numpy() for v in self.weights]\n    File \"c:\\Users\\yugan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py\", line 71, in numpy\n        return self.value.numpy()\n\n    NotImplementedError: numpy() is only available when eager execution is enabled.\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "\n",
    "env = RecordVideo(env, video_folder=\"acrobat-agent\", name_prefix=\"training\",\n",
    "                  episode_trigger=lambda x: x % 200 == 0)\n",
    "env = RecordEpisodeStatistics(env)\n",
    "\n",
    "# Creating a memory buffer\n",
    "memory=deque(maxlen=memory_buffer)\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100\n",
    "\n",
    "target_qnetwork.set_weights(qnetwork.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state,_=env.reset()\n",
    "    total_points=0\n",
    "    for t in range(max_num_timesteps):\n",
    "        state=np.array(state)\n",
    "        state=np.expand_dims(state, axis=0)\n",
    "        qvalues=qnetwork(state)\n",
    "        action=get_action(qvalues,epsilon)\n",
    "        next_state,reward,done,truncated,info=env.step(action)\n",
    "        memory.append(experience(state,action,reward,next_state,done))\n",
    "        state=next_state\n",
    "        total_points+=reward\n",
    "        \n",
    "        if check_update_conditions(t,5, memory):\n",
    "            experiences=get_experiences(memory)\n",
    "            train_step(experiences,gamma)\n",
    "        \n",
    "        state=next_state.copy()\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points=np.mean(total_point_history[-num_p_av:])\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Episode {i} : av_latest_points {av_latest_points}\")\n",
    "    \n",
    "    if av_latest_points >= 195.0:\n",
    "        qnetwork.save(\"acrobat model.keras\")\n",
    "        print(f\"Target Reached! on episode {i} and model saved\")\n",
    "        break\n",
    "    \n",
    "    epsilon=get_new_eps(epsilon)\n",
    "    logging.info(f\"episode-{num_episodes}\", info[\"episode\"])\n",
    "env.close()\n",
    "total_time=time.time()-start\n",
    "print(f\"Total Training Time: {total_time/60} minutes\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the training progress\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(total_point_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Points\")\n",
    "plt.title(\"Training Progress\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
